\chapter{Podstawy teoretyczne}

Problem kategoryzacji obrazów wpisuje się w dziedzinę rozpoznawania obrazów. Zadanie to polega na rozpoznaniu przynależności różnych rodzajów obiektów do pewnych klas\cite{Tad91} i jest częścią większego zagadnienia, które określa się jako uczenie maszynowe.

\section{Uczenie maszynowe}

Uczenie maszynowe jest zagadnieniem interdyscyplinarnym z pogranicza informatyki, statystyki i sztucznej inteligencji, które zajmuje się tworzeniem systemów mogących doskonalić się za pomocą dostarczanych danych.

Systemy te ulegają nieustannej modyfikacji, zmieniają swoje wewnętrzne parametry po to by osiągnąć korzyści takie jak zwiększenie efektywności lub wydajności działania. Ważną cechą jest to, że jakkolwiek zmiany te zachodzą na podstawie czynników zewnętrznych to jednak dokonują się w~systemie autonomicznie. System, który się uczy zmienia sam siebie na lepsze.\cite{CICHOSZ00}

\section{Uczenie nadzorowane i nienadzorowane}

W uczeniu maszynowym możemy wyróżnić dwa zasadnicze podejścia: uczenie nadzorowane (\emph{ang. supervised machine learning}) i nienadzorowane (\emph{ang. unsupervised machine learning}). Różnica pomiędzy nimi polega na obecności w uczeniu nadzorowanym tzw. nauczyciela, który stanowi źródło informacji trenującej. Posługując się analogiczną terminologią, system który podlega nauce określa się jako ucznia. Na rys. \ref{fig:supervised-learning} przedstawiono schemat uczenia maszynowego z użyciem nauczyciela.

\begin{figure}[h]
	\centering
	\includegraphics{graphics/01_podstawy_teoretyczne/supervised-learning.pdf}
	\caption{Uczenie maszynowe z nauczycielem \cite{CICHOSZ00}}
	\label{fig:supervised-learning}
\end{figure}

Uczeń otrzymuje od nauczyciela informację o tym jakiego komunikatu wyjściowego oczekuje w odpowiedzi na przykładowe informacje wejściowe. Mogą być one dostarczone np. w formie par składających się z wektorów danych oraz poprawnej odpowiedzi: $(x_{i}, y_{i})$. Na podstawie przykładów, system ma wyuczyć się przyporządkowywania danych do odpowiednich odpowiedzi. Innymi słowy, ma nauczyć się takiej funkcji $f$, dla której $f(x_{i})=y_{i}$, dla każdego $i$. Przykładami w uczeniu maszynowym mogą być różne obiekty, takie jak przedmioty, osoby, obserwacje.

Każdy przykład określany jest przez skończony oraz niepusty zbiór atrybutów $A=\lbrace a_{i}, a_{2}, \cdots, a_{n}\rbrace$. Atrybuty ze względu na rodzaj dziedziny możemy podzielić na: 
\begin{compactitem}
	\item nominalne, tj. takie, których dziedziny są zbiorami nieuporządkowanymi -- dla dwóch wartości możliwe jest określenie jedynie czy są równe czy nierówne,
	\item porządkowe, dla których dziedziny są zbiorami uporządkowanymi, czyli możliwe jest określenie relacji porządku liniowego na zbiorze wartości,
	\item liczbowe, których dziedziny są zdefiniowane jako zbiór liczbowy.
\end{compactitem}

W zależności od rodzaju odpowiedzi możemy wyróżnić dwa typowe rodzaje problemów, które stara się rozwiązać uczenie nadzorowane. Jeśli system ma przyporządkować do wektora wejściowego pojedynczą kategorię pochodzącą ze skończonego i dyskretnego zbioru kategorii, wówczas określamy ten problem jako problem kategoryzacji. Natomiast w przypadku przyporządkowywania odpowiedzi składającej się z jednej lub więcej zmiennych ciągłych, mówimy o problemie regresji.\cite{BISHOP06} Proces działania maszyny realizującej uczenie nadzorowane możemy podzielić na dwa etapy: etap treningu lub też nauki oraz etap polegający na rozwiązywania przynależności nowych wektorów wejściowych, który często określa się jako testowanie lub walidację.

W uczeniu nienadzorowanym nie występuje rola nauczyciela, a co za tym idzie, do systemu nie trafiają informacje treningowe. Dostępne są jedynie wektory wejściowe i na podstawie ich obserwacji system ma nauczyć się odpowiednich odpowiedzi.\cite{CICHOSZ00} Podejście to nie polega zatem na przyporządkowaniu wektorów do ustalonych wcześniej kategorii, lecz na wyznaczeniu ukrytej struktury w danych, które są nieopisane.\cite{VALPOLA} Wykorzystywane jest m.in. do rozwiązywania problemu klasteryzacji (\emph{ang. clustering}), który polega na znajdowaniu grup podobnych obiektów wśród danych wejściowych. Ponadto uczenie nienadzorowane wykorzystywane jest to określania rozkładu danych w przestrzeni wejściowej, co nazywamy szacowaniem rozkładu (\emph{ang. density estimation}) lub do rzutowania danych z przestrzeni wielowymiarowej do trójwymiarowej w celu dokonania wizualizacji.\cite{BISHOP06}

Na rys. \ref{fig:unsupervised-learning} przedstawiono uproszczony schemat mechanizmu uczenia nienadzorowanego. Algorytm uczący dokonuje podziału zestawu danych na klastry, które następnie muszą zostać zinterpretowane przez człowieka. W zaprezentowanym przykładzie klastry zostały nazwane na podstawie analizy ich zawartości. W rzeczywistych problemach zadanie nadania etykiet wytworzonym klastrom może okazać się trudne i może wymagać dostrojenia parametrów algorytmu by identyfikacja klastrów była możliwa.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.77]{graphics/01_podstawy_teoretyczne/unsupervised-learning.pdf}
	\caption{Uczenie maszynowe nienadzorowane \cite{CASEY}}
	\label{fig:unsupervised-learning}
\end{figure}

Dane wejściowe z etapu walidacji mogą się różnić od przykładowych danych służących do wytrenowania systemu. Umiejętność udzielania poprawnej odpowiedzi na komunikaty, które różnią się od przykładowych danych określamy jako generalizacja.\cite{BISHOP06}
%TODO napisac o tym w kontekscie pracy, co raczej sie nadaje a co raczej nie

Wśród dostępnych publikacji na temat kategoryzacji obrazów przeważają rozwiązania oparte na uczeniu nadzorowanym lub pól-nadzorowanym.\cite{MELE06}\cite{CHEN04}\cite{Vitaladevuni13}\cite{LUO11} Istnieje jednak kilka rozwiązań opartych na uczeniu nienadzorowanym, m.in. w  Dai D., Wu T., Zhu S-c., \emph{Discovering Scene Categories by Information Projection and Cluster Sampling}\cite{DAI10} gdzie zaprezentowano metodę kategoryzacji sceny opartą na uczeniu nienadzorowanym, lub Huang Y., Liu Q., Lv F., Gong Y., Dimitris N., \emph{Unsupervised Image Categorization by Hypergraph Partition}\cite{HUANG11}, gdzie klastrowanie obrazów sformowano jako problem podziału hipergrafu.

\section{Rozpoznawanie wzorców}

%TODO sprawdzic wzory

Celem rozpoznawania wzorców (\emph{ang. pattern recognition}) jest stworzenie symbolicznego opisu dla zawartości jedno lub wielowymiarowego sygnału cyfrowego takiego jak np. cyfrowy obraz graficzny, sygnał mowy, lub obraz z kamery, a następnie przyporządkowanie do niego klasy lub instancji klasy. Opis ten może być zrealizowany w różnej formie: funkcji, obiektów, ruchu, wyrazów lub struktur.

Wzorcem nazywamy zbiór cech, który tworzy ilościowy oraz jakościowy opis obiektu. Wzorce zapisujemy najczęściej jako wektory, ciągi lub drzewa. Wzór \ref{pattern_vector} przedstawia wektor cech, gdzie $x_{i}$ reprezentuje $i$-tą cechę, a $n$ oznacza ilość wszystkich cech powiązanych ze wzorcem.\cite{GONZALES01}
\begin{equation} 
\label{pattern_vector} 
\overrightarrow{x}=
\begin{bmatrix}x_1\\
x_2\\
\vdots\\
x_n  
\end{bmatrix}
\end{equation}

Zbiór wzorców charakteryzujący się podobnymi wektorami cech nazywamy klasą wzorców i oznaczamy jako $\omega_1, \omega_2, \cdots, \omega_{W}$, gdzie dolny indeks określa numer klasy, a $W$ określa ilość klas. Jako rozpoznanie wzorca $\omega$ określa się przyporządkowanie wzorcowi jego klasy: $\overrightarrow{x} \rightarrow \omega$. Przestrzeń wektorów cech $X$ zostaje przekształcona w przestrzeń klas wzorców $\Omega$.

Zazwyczaj systemy rozpoznawania wzorców składają się z dwóch części: ekstraktora cech oraz klasyfikatora (rys. \ref{fig:pattern-recongnition}). Procesy ekstrakcji oraz klasyfikacji zostaną omówione w następnym podrozdziałach.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.0]{graphics/01_podstawy_teoretyczne/pattern-recongnition}
	\caption{ Budowa systemu rozpoznawania wzorców, składającego się z ekstraktora cech oraz klasyfikatora \cite{FUKUNAGA90} }
	\label{fig:pattern-recongnition}
\end{figure}

W systemach analizy obrazów możemy oprócz rozpoznania zdefiniować również dwa inne pojęcia: identyfikację oraz weryfikację. Gdy nie dysponujemy opisami klas wzorców, a jedynie przechowujemy wzorce referencyjne, celem systemu jest porównanie aktualnego wzorca z bazą referencyjną. Jeśli na podstawie określonej odległości pomiędzy parami wzorców system jest w stanie dokonać wyboru dokładnie jednego wzorca referencyjnego, to określamy takie rozwiązanie jako identyfikację. Natomiast jeśli możemy wybrać przynajmniej jeden ze wzorców referencyjnych to określamy takie rozwiązanie jako weryfikację.

\section{Selekcja cech}

%TODO sprawdzic wzory

W przypadku dużych i skomplikowanych obiektów, takich jak obrazy, proces uczenia może okazać się bardzo skomplikowany i czasochłonny. Konieczne jest przeprowadzenie procesu selekcji cech obrazu (\emph{ang. feature selection}), który polega na znalezieniu podzbioru cech opisujących obiekt w najlepszy sposób i zapewniający najwyższą jakość modelu klasyfikacji. Celem selekcji cech jest redukcja wymiarów przestrzeni cech, zwiększenie szybkości działania algorytmu uczącego, zwiększenie dokładności klasyfikacji oraz osiągnięcie łatwiejszych do zrozumienia rezultatów uczenia.\cite{MOTODA98}

Wyselekcjonowane cechy powinny się charakteryzować następującymi właściwościami\cite{STRUMIL}:

\begin{compactitem}
	\item \emph{dyskryminacja} -- cechy powinny przyjmować jak najbardziej odmienne wartości dla obiektów z różnych klas,
	\item \emph{niezawodność} -- cechy powinny przyjmować podobne wartości dla wszystkich obiektów danej klasy,
	\item \emph{niezależność} -- cechy powinny nie być ze sobą skorelowane,
	\item \emph{małoliczność} -- im mniejsza liczba cech tym złożoność systemu jest mniejsza.
\end{compactitem}

Wzór \ref{correlation} przedstawia współczynnik korelacji cech $x$ i $y$. Jeśli współczynnik korelacji jest bliski $1$ lub $-1$ to cechy $x$ i $y$ są ze sobą silnie skorelowane i należy jedną z nich odrzucić.
\begin{equation} 
\label{correlation} 
\hat{\sigma}_{xy}= \frac{\frac{1}{P}\sum\limits_{i=1}^P(x_i-\mu_x)(y_i-\mu_y)}{ \hat{\sigma}_x\hat{\sigma}_y }
\end{equation} gdzie $P$ -- liczba klasyfikowanych obiektów, $\mu$ -- wartość średnia, $\sigma$ -- odchylenie standardowe zbioru cech.

Miarę separacji można określić jako:

\begin{equation}
	\hat{D}_{xjk} = \frac{|\mu_xj - \mu_xk|}{\sqrt{\hat{\sigma}^2_xj \hat{\sigma}^2_xk}}
\end{equation}

Duża wartość tej miary świadczy o dobrej separacji pomiędzy klasami $j$ i $k$.

Badanie jakości klasyfikacji dla wszystkich możliwych kombinacji podzbioru cech dla dużej liczby $N$ wszystkich cech jest bardzo kosztowne, dlatego w praktyce zbiór cech często dobierany jest intuicyjnie.\cite{STRUMIL}

%TODO Opis mRMR, CFS, http://en.wikipedia.org/wiki/Feature_selection

Po poprawnej selekcji cech, możliwe powinno się stać stworzenie takiego modelu, w którym cechy nie dające informacji potrzebnej do klasyfikacji lub takie, które są nadmiarowe lub pogarszają jakość klasyfikacji zostały wyeliminowane. Dążymy przy tym do osiągnięcia najmniejszego możliwego wektora cech. Opierając się na mniejszym, mniej wymiarowym wektorze cech, potrzeba mniej przykładów do osiągnięcia dobrych wyników klasyfikacji.

\section{Ekstrakcja cech}

Obrazy w formie map bitowych reprezentowane są w pamięci komputera w postaci macierzy pikseli. Ilość danych jest dla klasyfikacji zbyt duża i zawiera wiele niepotrzebnych informacji. Dlatego przed przystąpieniem do procesu klasyfikacji należy wydobyć z obrazu pewne cechy. Proces przetwarzania obrazu w celu wydobycia cech określa się jako ekstrakcję cech (\emph{ang. feature extraction}).

Warto zauważyć, że ekstrakcja cech jest procesem odmiennym od omówionej wcześniej selekcji cech. Selekcja cech polega na znalezieniu podzbioru cech, natomiast ekstrakcja cech na tworzeniu nowych cech z funkcji oryginalnych cech. Proces ekstrakcji może być skomplikowany i składać się ze znacznej liczby operacji na obrazie. Rezultat ekstrakcji cech często określany jest jako \emph{deskryptor cechy} lub \emph{wektor cechy}. 

Można wyróżnić wiele cech używanych w procesach przetwarzania obrazów, np. rozkład kolorów w formie histogramu, obecność niektórych elementów lub kształtów, liczba występowania podobnych obiektów, wielkość kształtów, morfologia, tekstura. Przed ekstrakcją cech często warto przeprowadzić przetwarzanie wstępne (\emph{ang. preprocessing}) celem uzyskania obrazów o lepszych parametrach do ekstrakcji. Często przy przetwarzaniu wstępnym wykorzystuje się normalizację kolorów oraz korekcję gamma.

\section{Przegląd algorytmów do ekstrakcji cech}

Wśród popularnych metod ekstrakcji cech, stosowanych do rozwiązania problemu klasyfikacji i kategoryzacji obrazów należy wymienić: SIFT (\emph{ang. Scale-invariant Feature Transform})\cite{SIFT99}, SURF (\emph{ang. Speeded Up Robust Features}), GIST\cite{GIST09}, LBP (\emph{ang. Local Binary Patterns}), HOG (\emph{ang. Histograms of Oriented Gradients}). Poszczególne algorytmy zostały opisane poniżej.

\subsection{SIFT -- Scale-invariant Feature Transform}

Algorytm SIFT służy do wykrywania oraz opisu lokalnych cech obrazu, został opublikowany przez Dawida Lowe w 1999 roku.\cite{SIFT99} Punkty charakterystyczne wykrywane przez SIFT są niezmiennicze względem położenia, rotacji, skali, zmiany oświetlenia oraz są odporne na zakłócenia. Zastosowanie algorytmu SIFT do rozpoznawania obiektów pozwala na detekcję częściowo zasłoniętych obiektów. Jest to jeden z najpopularniejszych algorytmów do ekstrakcji punktów charakterystycznych.

Działanie algorytmu rozpoczyna się od wyszukiwania potencjalnych punktów poprzez przetworzenie obrazu filtrem Gaussa w różnych skalach. Stanowi to implementację tzw. przestrzeni skal (\emph{ang. skale space}). Przestrzeń skal definiujemy jako funkcję $L(x, y, \sigma)$. Wzór \ref{sift_scale_space} przedstawia reprezentację przestrzeni skal po przetworzeniu obrazu $I(x, y)$:
\begin{equation} 
\label{sift_scale_space} 
L(x, y, \sigma) = G(x, y, \sigma) \ast I(x, y)
\end{equation} gdzie $G(x, y, \sigma)$ -- Gaussian, $I(x, y)$ -- obraz wejściowy, $\ast$ -- operator splotu.
\begin{equation} 
\label{sift_gaussian} 
G(x, y, \sigma) = \frac{1}{2\pi\sigma^2}e^{-(x^2+y^2)/2\sigma^2}
\end{equation} 

W celu optymalnego wykrywania stabilnych punktów charakterystycznych, zaproponowano użycie ekstremum przestrzeni skal dla funkcji DOG (\emph{ang. difference-of-Gaussian}) w splocie z obrazem, oznaczone jako $D(x, y, \sigma)$.

Różnicowy filtr Gaussa (DOG) dla dwóch skal rozdzielonych współczynnikiem $k$ można zapisać jako:
\begin{equation} 
\label{sift_dog} 
G(x, y, k\sigma) - G(x, y, \sigma) 
\end{equation} 

Stąd wynik spotu obrazu z filtrem DOG można zapisać jako:
\begin{equation} 
\label{sift_splot} 
	\begin{gathered}
		D(x, y, \sigma) = [G(x, y, k\sigma) - G(x, y, \sigma)] \ast I(x, y) \\
		D(x, y, \sigma) = L(x, y, k\sigma) - L(x, y, \sigma)
	\end{gathered}
\end{equation}

Na rys. \ref{fig:sift-gaussian-to-dog} przedstawiono efektywny sposób konstruowania $D(x, y, \sigma)$. Obraz wejściowy fitrowany jest iteracyjnie filtrem Gaussa w różnych skalach rozdzielonych stałym współczynnikiem $k$ w celu wytworzenia obrazów oddalonych od siebie o współczynnik $k$ w przestrzeni skali (lewa kolumna).

Przestrzeń skal jest dzielona na oktawy, które reprezentują serię map odpowiedzi filtra uzyskane przez obliczenie splotu obrazu z filtrem. Zwiększenie skali o oktawę oznacza zwiększenie wygładzania o 2. Każda oktawa jest dzielona na liczbę całkowitą $s$, określającą liczbę interwałów, więc $k=2^{1/s}$. Należy zatem wyprodukować $s+3$ obrazów dla każdej oktawy, tak aby końcowe ekstremum pokrywało całą oktawę. Obrazy z sąsiednich skal są następnie odejmowane od siebie w celu otrzymania różnicy gaussianów DOG. Procedurę powtarza się próbkując obraz gaussowski poprzez usunięcie co drugiego wiersza i kolumny obrazu.\cite{LOWE04}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{graphics/01_podstawy_teoretyczne/sift-gaussian-to-dog.pdf}
	\caption{Proces tworzenia różnicy Gaussianów w algorytmie SIFT \cite{LOWE04}}
	\label{fig:sift-gaussian-to-dog}
\end{figure}

W celu wykrycia lokalnych ekstremów, punkty są porównywane z sąsiadami w otoczeniu 3x3x3, tj. po dziewięć sąsiadów w skalach poniżej i powyżej oraz z ośmioma sąsiadami w aktualnej skali. Punkt jest przyjmowany jako kandydat na punkt charakterystyczny tylko wówczas gdy ma wartość mniejszą lub większą od wszystkich 26 punktów sąsiednich.\cite{LOWE04} Rys. \ref{fig:sift-gaussian-min-max} przedstawia porównanie punktu, oznaczonego na schemacie za pomocą "X", z punktami sąsiednimi.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{graphics/01_podstawy_teoretyczne/sift-gaussian-min-max.pdf}
	\caption{Porównanie punktu z sąsiadami \cite{LOWE04}}
	\label{fig:sift-gaussian-min-max}
\end{figure}

Po znalezieniu kandydatów następuje lokalizacja punktów charakterystycznych. Proces ten odbywa się poprzez interpolację ekstremów dla kandydatów wyznaczonych w poprzednim punkcie za pomocą funkcji kwadratowej 3D:
\begin{equation} 
\label{sift_3d} 
D(x) = D + \frac{\partial D^T}{\partial x} x + \frac{1}{2}x^T \frac{\partial^2 D}{\partial x^2} x
\end{equation}

Dokładne położenie ekstremum wyznacza się na podstawie pochodnej funkcji $D(x)$:
\begin{equation} 
\label{sift_pochodna_D} 
\hat{x} = -\frac{\partial^2 D^-1}{\partial x^2}\frac{\partial D}{\partial x}
\end{equation}

Wartość funkcji w punkcie ekstremum służy do odrzucania niestabilnych punktów ze względu na mały kontrast. Wzór można uzyskać poprzez podstawienie wzoru \ref{sift_pochodna_D} do \ref{sift_3d}:
\begin{equation} 
\label{sift_Dx} 
D(\hat{x}) = D + \frac{1}{2} \frac{\partial D^T}{\partial x} \hat{x}
\end{equation}

Funkcja DOG będzie mieć dużą odpowiedź wzdłuż krawędzi. Za Harris i Stephens (1988), punkty ekstremum będące odpowiedzią wzdłuż krawędzi na obrazie są odrzucane na podstawie warunku \ref{sift_warunek}:
\begin{equation} 
\label{sift_warunek} 
\frac{\Tr(\boldsymbol{H})^2}{\Det(\boldsymbol{H})} < \frac{(r+1)^2}{r}
\end{equation} gdzie Hessian ma postać macierzy:
\begin{equation} 
\label{sift_hessian} 
\boldsymbol{H} = 
	\begin{bmatrix}
		D_{xx} & D_{xy} \\
		D_{xy} & D_{yy}
	\end{bmatrix}
\end{equation} oraz $r$ jest stosunkiem największej wartości własnej macierzy do najmniejszej.

Orientacja punktów wyznaczana jest na podstawie jednego z obrazów gaussowskich, którego skala odpowiada skali danego punktu charakterystycznego. Dla każdej próbki obrazu $L(x, y)$ obliczane są moduł gradientu $m(x, y)$ oraz orientacja $\theta(x, y)$:
\begin{equation} 
\label{sift_gradient_magnitude} 
m(x, y) = \sqrt{(L(x + 1, y) - L(x - 1, y))^2 + (L(x, y + 1) - L(x, y - 1))^2}
\end{equation}

\begin{equation} 
\label{sift_orientation} 
\theta(x, y) = \tan^{-1}((L(x, y + 1) - L(x, y - 1))/L(x + 1, y) - L(x - 1, y)))
\end{equation}

Na podstawie orientacji punktów, w otoczeniu punktu charakterystycznego budowany jest histogram orientacji. Maksimum w histogramie i lokalne maksima o wartościach powyżej 80\% największego maksimum lokalnego określają orientację punktu charakterystycznego. Przyjmuje się, że histogram posiada 36 pól, a dokładne położenie maksimum interpoluje się za pomocą paraboli korzystając z wartości sąsiednich histogramu.

Dzięki temu, że wszystkie cechy punktów charakterystyczne są mierzone w odniesieniu do tak wyznaczonej orientacji, opis jest niezmienniczy względem rotacji.

SIFT do opisu cech wykorzystuje moduł gradientu oraz orientację z otoczenia 16x16 dla danego punktu charakterystycznego. Obszar ten jest dzielony na podregiony 4x4, w których również wyznacza się wypadkowe histogramy orientacji. W każdym obszarze dla 8 orientacji wyznacza się wypadkowy moduł gradientu na podstawie modułów poszczególnych punktów. Na rys. \ref{fig:sift-descriptor} został przedstawiony przykład wyznaczenia wypadkowego gradientu dla otoczenia 8x8 i podregionów 2x2.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1]{graphics/01_podstawy_teoretyczne/sift-descriptor.pdf}
	\caption{Wyznaczenie deskryptora SIFT o rozmiarze 2x2 z otoczenia 8x8 \cite{LOWE04}}
	\label{fig:sift-descriptor}
\end{figure}

W celu rozpoznania obiektów za pomocą algorytmu SIFT należy najpierw wyznaczyć punkty charakterystyczne i odpowiadające im wektory cech lokalnych, a następnie dopasować punkty charakterystyczne z obrazu wejściowego do punktów w bazie wzorców. Do identyfikacji grup dopasowanych punktów, które określają ten sam obiekt wykorzystuje się transformatę Hougha. Weryfikacji cech dokonuje się z wykorzystaniem metody najmniejszych kwadratów. W trakcie obliczeń wyznaczane są parametry związane z transformacją pomiędzy układem związanym z wzorcem a układem związanym z obiektem na badanym obrazie.

\subsection{SURF -- Speeded Up Robust Features}

SURF jest kolejnym algorytmem do wykrywania lokalnych cech obrazu. Został zaproponowany w 2006 roku przez Herberta Bay'a i jest częściowo wzorowany na algorytmie SIFT.\cite{BAY08} Algorytm jest odporny zarówno na zmiany skali jak i zmiany rotacji. Wykorzystywany jest do rozpoznawania obiektów jak i rekonstrukcji scen 3D. Typowa implementacja jest nawet kilka razy szybsza od algorytmu SIFT.\cite{SCHWEIGER09}

W celu optymalizacji działania filtrów splotowych zastosowano obrazy całkowe (\emph{ang. integral images}). Pozycja w obrazie całkowym $I_{\Sigma}(\boldsymbol{x})$ w punkcie $\boldsymbol{x}=(x, y)^T$ reprezentuje sumę intensywności wszystkich pikseli wejściowego obrazu $I$ wewnątrz prostokąta pomiędzy początkiem układu współrzędnych a $x$.
\begin{equation} 
\label{surf_integral_image} 
I_{\Sigma}(\boldsymbol{x}) = \sum\limits_{i=0}^{i \leq x} \sum\limits_{j=0}^{j \leq y} I(i,j)
\end{equation}

Zastosowanie obrazu całkowego (\emph{ang. integral image}) pozwala ograniczyć ilość operacji dla sumowania intensywności oraz uniezależnić czas obliczeń od wielkości filtra.\cite{BAY08}

Ze względów wydajnościowych, wyszukiwanie punktów charakterystycznych oparte zostało na Hessianie. Dla punktu $\boldsymbol{x}=(x, y)$ na obrazie $I$, Hessian $H(\boldsymbol{x}, \sigma)$ w punkcie $\boldsymbol{x}$ i w skali $\sigma$ został zdefiniowany we wzorze \ref{surf_hessian}.
\begin{equation} 
\label{surf_hessian} 
H(\boldsymbol{x}, \sigma) = 
	\begin{bmatrix}
		L_{xx}(\boldsymbol{x}, \sigma) & L_{xy}(\boldsymbol{x}, \sigma) \\
		L_{xy}(\boldsymbol{x}, \sigma) & L_{yy}(\boldsymbol{x}, \sigma)
	\end{bmatrix}
\end{equation} gdzie $L_{xx}(\boldsymbol{x}, \sigma)$ -- splot drugiej pochodnej Gaussiana $\frac{\partial^2}{\partial x^2} g(\sigma)$ z obrazem $I$ w punkcie $\boldsymbol{x}$.

O ile zastosowanie Gaussianów jest optymalne\cite{LINDENBERG90}, to jednak ma obniżoną powtarzalność przy rotacji dla nieparzystych wielokrotności $\frac{\pi}{2}$. Korzystając z tych obserwacji, w algorytmie SURF zaproponowano przybliżenie oparte na filtrze pudełkowym, w celu zwiększenia wydajności. Na rys. \ref{fig:surf-box-filters} zostało przedstawione porównanie laplacianów $L_{yy}$ (a), oraz $L_{xy}$ (b), oraz odpowiadające im przybliżenia: $D_{yy}$ (c) oraz $D_{xy}$ (d).

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.7]{graphics/01_podstawy_teoretyczne/surf-box-filters.pdf}
	\caption{Porównanie filtrów Gaussa z filtrami pudełkowymi 9x9 (skala $\sigma = 1.2$) \cite{BAY08}}
	\label{fig:surf-box-filters}
\end{figure}

Wyznacznik przybliżonej wartości Hessiana $H$ można zapisać jako:
\begin{equation} 
\label{surf_hessian_det}
\det(H_{approx}) = D_{xx} D_{yy} - (w D_{xy})^2
\end{equation}

Waga odpowiedzi filtra $w$ służy do zrównoważenia wyrażenia, dla zachowania energii pomiędzy maską Gaussa a przybliżoną maską Gaussa.
\begin{equation} 
\label{surf_hessian_approx_w}
w = \frac{|L_{xy}(1.2)|_F |D_{yy}(9)|_F}{|L_{yy}(1.2)|_F |D_{xy}(9)|_F} = 0.912... \simeq 0.9
\end{equation} gdzie $|A|_F$ -- norma Froberiusa (wzór \ref{surf_froberius}).
\begin{equation} 
\label{surf_froberius}
|A|_F = \sqrt{\sum\limits_{i=1}^m \sum\limits_{j=1}^n |a_{ij}|^2}
\end{equation}

Ze względu na teoretyczną dokładność, wagi $w$ powinny zmieniać się wraz ze skalą, jednakże w praktyce nie ma to dużego wpływu na precyzję wyników, w związku z tym przyjmuje się stałe $w = 0.9$. Przybliżone wyznaczniki Hessianów reprezentują odpowiedź obszarów spójnych ({ang. blob}) w punkcie $x$. Odpowiedzi filtra są normalizowane tak aby zachować stałą normę Frobeniusa dla każdego rozmiaru filtra.

W przypadku algorytmu SIFT, w celu uzyskania odpowiedniej skali, filtrowaliśmy obraz iteracyjnie tym samym filtrem. W algorytmie SURF, dzięki zastosowaniu obrazów całkowych, można zastosować bezpośrednio na obrazie wejściowym filtr pudełkowy o odpowiednim rozmiarze. Dzięki temu obrazy o różnych skalach można uzyskać nawet równolegle. 

Konstrukcja przestrzeni skal rozpoczyna się od filtra 9x9, który oblicza odpowiedź obszaru spójnego obrazu dla najmniejszej skali. Każda oktawa jest dzielona na stałą liczbę poziomów skalowania. Ze względu na dyskretny charakter obrazów całkowych, minimalna różnica skali pomiędzy dwoma kolejnymi poziomami zależy od długości $l_0$, która określa rozmiar obszaru o jednakowych wartościach (1, -1, -2) na filtrze. W przypadku filtra 9x9 wartość ta wynosi 3.

Dla dwóch kolejnych poziomów wartość tą należy zwiększyć przynajmniej o 2 piksele oraz zapewnić wystąpienie środkowego piksela, w rezultacie wielkość filtra zwiększa się o 6 pikseli pomiędzy poziomami. Porównanie filtrów 9x9 z filtrami 15x15 przedstawiono na rys. \ref{fig:surf-9-to-15}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.5]{graphics/01_podstawy_teoretyczne/surf-9-to-15.pdf}
	\caption{Porównanie filtrów pudełkowych 9x9 z filtrami 15x15: (a) $D_{yy}$, (b) $D_{xy}$ ($\sigma = 1.2$) \cite{BAY08}}
	\label{fig:surf-9-to-15}
\end{figure}

Dla kolejnych oktaw różnica pomiędzy poziomami jest zwiększana dwukrotnie, tj. wynosi 12 dla drugiej oktawy, 24 dla trzeciej, 48 dla czwartej itd. Algorytm może analizować wiele oktaw, jednak z każdą kolejną, ilość wykrytych punktów charakterystycznych maleje. Na rys. \ref{fig:surf-octaves} przedstawiono wielkości filtrów dla kilku kolejnych oktaw. Filtry w kolejnych oktawach nachodzą na siebie tak aby pokryć wszystkie możliwe skale.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.7]{graphics/01_podstawy_teoretyczne/surf-octaves.pdf}
	\caption{Wielkości filtrów pudełkowych dla kilku kolejnych oktaw \cite{BAY08}}
	\label{fig:surf-octaves}
\end{figure}

W celu zlokalizowania punktów charakterystycznych wykorzystywana jest szybka metoda usuwania niemaksymalnych pikseli (\emph{ang. Non-Maximum Suppresion}) zaproponowana przez Neubecka i Van Goola\cite{NEUBECK06}. Podobnie jak w algorytmie SIFT, punkty te są wyszukiwane w otoczeniu 3x3x3.

Deskryptor SURF opisuje rozkład intensywności dla punktów charakterystycznych. W celu czasu obliczeń zastosowano odpowiedzi falek Haara w kierunkach $x$ i $y$ dla otoczenia punktów charakterystycznych o promieniu $6s$, gdzie $s$ jest skalą na której został wykryty punkt charakterystyczny. Wielkości filtrów Haara są zależne od skali stąd ich wielkość została określona na $4s$. Na rys. \ref{fig:surf-haar-wavelet} przedstawiono filtr Haara dla kierunku $x$ (a) oraz $y$ (b).

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.5]{graphics/01_podstawy_teoretyczne/surf-haar-wavelet.pdf}
	\caption{Filtry falkowe Haara do obliczenia odpowiedzi w kierunku $x$ (a) oraz $y$ (b) \cite{BAY08}}
	\label{fig:surf-haar-wavelet}
\end{figure}

Odpowiedzi filtra Haara sa następnie filtrowane za pomocą filtra Gaussa z $\sigma = 2s$ w celu wyeliminowania odkształceń. Na podstawie otrzymanych punktów, obliczane są sumy odpowiedzi w oknach o wielkości $\frac{\pi}{2}$, a następnie najdłuższy wektor z wszystkich okien definiuje się jako orientację punktu charakterystycznego.

Ekstrakcja rozpoczyna się od stworzenia kwadratowego obszaru wokół punktu charakterystycznego o wielkości $20s$. Obszar jest dzielony na podregiony 4x4, a następnie dla każdego podregionu, obliczane są odpowiedzi filtra Haara dla równo rozmieszczonych punktów w 5 rzędach i 5 kolumnach. Odpowiedzi  w kierunku $x$ oznaczane są jako $d_x$, natomiast odpowiedzi w kierunku $y$ oznaczane są jako $d_y$. Odpowiedzi $dx$ i $dy$ są następnie sumowane dla każdego podregionu i zostają umieszczone w wektorze cech. W celu określenia polaryzacji zmian intensywności, ekstraktuje się również sumę wartości bezwzględnych $|d_x|$ oraz $|d_y|$. W rezultacie każdy podregion otrzymuje czterowymiarowy wektor $\boldsymbol{v}$ określający strukturę intensywności.
\begin{equation} 
\label{surf_v_vector} 
\boldsymbol{v} = (\sum{d_x}, \sum{d_y}, \sum{|d_x|}, \sum{|d_y|})
\end{equation}

W rezultacie otrzymujemy wektor o długości równej 64. Odpowiedzi filtra Haara są niezmiennicze względem oświetlenia, natomiast niezmienniczość względem kontrastu jest otrzymywana poprzez wyliczenie wektora jednostkowego dla deskryptora.

\subsection{Modele GIST}

Studia nad percepcją pokazały, że obserwator jest w stanie jednym spojrzeniem rozpoznać scenę z realnego świata. W czasie tego szybkiego procesu, w układzie wzrokowym człowieka (rys. \ref{fig:gist-vision-system}) tworzona jest przestrzenna reprezentacja świata zewnętrznego, która jest na tyle bogata, że pozwala zrozumieć sens sceny, rozpoznać kilka obiektów oraz zwrócić uwagę na istotne szczegóły. Ta reprezentacja, którą można odnieść zarówno do funkcji niskopoziomowych, takich jak kolor lub częstotliwość, jak i informacji wysokiego poziomu, takich jak obiekty lub aktywacja wiedzy semantycznej określana jest jako gist (ang. istota, sedno).

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.0]{graphics/01_podstawy_teoretyczne/gist-vision-system.pdf}
	\caption{Schemat układu wzrokowego człowieka \cite{GRAY18}}
	\label{fig:gist-vision-system}
\end{figure}

W czasie nie większym niż 100~ms, człowiek jest w stanie rozpoznać podstawowe kategorie obrazu, takie jak określenie miejsca, tj. czy obraz przedstawia ulicę, wnętrze mieszkania, las lub inną lokalizację\cite{POTTER76}, jak również jest w stanie rozpoznać układ przestrzenny zdjęcia, nawet jeśli jest ono nieostre.\cite{SCHYNS94} Potrafi także zapamiętać kilka obiektów, ich kontekst oraz niskopoziomową charakterystykę obszarów, które są najlepiej zauważalne.\cite{OLIVA05}

Cecha GIST w założeniu jest niskopoziomową reprezentacją sceny, która nie wymaga żadnej formy segmentacji. Podejście to nie jest alternatywą dla lokalnych cech obrazu ale może stanowić dodatkową pomoc przy rozpoznawaniu obiektów na nieuporządkowanej scenie.\cite{OLIVA06}

%-- Oliva Torralba 2000
%=================================================================================
Nie istnieje jeden model GIST. Oliva i Torralba w 2000 roku zaproponowali model, w którym rozkład przestrzenny widma został opisany przy pomocy okienkowej transformaty Fouriera (WFT), a następnie otrzymany wektor cech został zredukowany poprzez zastosowanie analizy głównych składowych (\emph{ang. PCA -- Principal Component Analysis}).\cite{OLIVA00} Okienkowa transformata Fouriera została określona przy pomocy wzoru \ref{gist_wft}:
\begin{equation} 
\label{gist_wft} 
I(x, y, f_x, f_y) = \sum\limits_{x^\prime, y^\prime = 0}^{N-1} i(x^\prime, y^\prime)h_r(x^\prime - x, y^\prime - y)e^{-j2\pi(f_xx^\prime+f_yy^\prime)}
\end{equation} gdzie $h_r(x^\prime, y^\prime)$ -- okno czasowe Hamminga

Wyznaczenie cech dla pojedynczego obrazu jest dla metody Oliva i Torralba bardzo szybkie i wynosi poniżej 1ms.

%-- Renninger Malik 2004
%=================================================================================
Renninger i Malik w 2004 zaproponowali inne rozwiązanie oparte na filtrach Gabora oraz ekstrakcji tekstonów. W zaproponowanym modelu wykorzystano pierwszą i drugą pochodną Gaussiana. Filtr składa się z dwóch faz -- parzystej i nieparzystej, opisanych we wzorze \ref{gist_gabor}.
\begin{equation} 
\label{gist_gabor} 
	\begin{gathered}
		f_{odd}(x, y) = {G^\prime}_{\sigma_1}(y) G_{\sigma_2}(x) \\
		f_{even}(x, y) = {G^{\prime\prime}}_{\sigma_1}(y) G_{\sigma_2}(x) 
	\end{gathered}
\end{equation} gdzie $G_\sigma(x)$ -- Gaussian z odchyleniem standardowym $\sigma$

Współczynniki $\sigma_1$ i $\sigma_2$ są miarami wydłużenia filtra. Filtry są budowane w 3 skalach dla zachowania selektywności częstotliwości oraz w 6 orientacjach dla selektywności orientacji. W rezultacie powstaje 36 filtrów. Podczas analizy tekstury, obliczany jest splot obrazu z zespołem filtrów, co w rezultacie daje wektor odpowiedzi $I \ast f(x_0, y_0)$ charakteryzujący fragment obrazu wyśrodkowany w $(x_0, y_0)$. 

Podobne tekstury na analizowanych obrazach powtarzają się bardzo często. W celu określenia jakie cechy są najbardziej powszechne użyto klastrowania K-średnich i otrzymano wektor składający się z 100 prototypowych odpowiedzi, które nazywamy \emph{uniwersalnymi tekstonami}. Ze względu na konieczność wyliczenia tekstonów rozwiązanie to jest dużo wolniejsze i wyliczenie cech dla obrazu zajmuje ok. 7 sekund.\cite{VISWANATHAN08}

%-- Siegian Itti 2007
%=================================================================================
W 2007 roku Siegian i Itti zaproponowali model składający się z biologicznie wiarygodnych cech określających orientację, kolor oraz intensywność.\cite{SIAGIAN07} Metoda ta powstała na kanwie badań nad percepcją, które wykazały, że obserwator jest w stanie określić ogólne, semantyczne atrybuty zdjęcia na podstawie ekspozycji nie przekraczającej 100~ms. Natomiast odpowiedź na bardziej specyficzne pytania, takie jak czy na zdjęciu znajduje się zwierzę, obserwator jest w stanie określić w czasie 28~ms\cite{THORPE95}, nawet wówczas gdy jego uwaga jest skupiona na innym zadaniu. Gist obrazu może być określany w obszarach mózgu, które odpowiadają za \emph{miejsca}, rozumiane jako typy sceny z określonym układem przestrzennym.\cite{EPSTEIN00} Układ przestrzenny oraz kolor zostały zostały doświadczanie określone jako cechy obrazu wpływające na dostrzeganie gistu sceny.\cite{OLIVA-SCHYNS00}

Model Siegiana i Itti opiera się na na \emph{saliency model} zaproponowanym wcześniej przez Itti. W modelu tym, obraz wejściowy jest filtrowany do wielu niskopoziomowych kanałów cech, w wielu skalach przestrzennych. Kanały takie jak kolor, orientacja i intensywność mają wiele podkanałów, które z kolei wyróżniają 9 poziomów skali od stosunku 1:1 (poziom 0) do 1:256 (poziom 8), wygładzone iteracyjnie filtrem Gaussa 5x5.

Dla każdego podkanału $i$ wykonywane są operacje porównania wartości centralnego punktu z jego sąsiedztwem, pomiędzy mapami wyjściowymi dla filtrów $O_i(s)$ w różnych skalach $s$. Mapy cech określamy jako $M_i(c, s)$, gdzie $c \in \{2, 3, 4\}$ -- skala centralna, $s = c + d$, $d \in \{3, 4\}$ -- skala otaczająca. Różnica między skalami zapisywana jest jako $\ominus$ i jest realizowane przez interpolację do skali centralnej i wyliczenie różnicy bezwzględnej. Zastosowanie skal $s$ oraz $c$ umożliwia zachowanie niezmienniczości względem oświetlenia.

Dla kanałów koloru i intensywności mamy:
\begin{equation} 
\label{gist_siegian_color_intensity}
M_i(c, s) = |O_i(c) \ominus O_i(s)| = |O_i(c) - Interp_{s-c}(O_i(s))|
\end{equation}

Kanał orientacji tworzony jest za pomocą filtrów Gabbora na obrazie w skali szarości, w czterech różnych kątach $\theta_i \in \{0, 45, 90, 135^\circ\}$ oraz dla czterech różnych skalach przestrzennych $c \in \{0, 1, 2, 3\}$, co daje łącznie 16 podkanałów. Mapy cech dla orientacji $M_i(c)$ określamy jako:
\begin{equation} 
\label{gist_siegian_orientation}
M_i(c) = Gabor(\theta_i, c)
\end{equation}

Kanały koloru i intensywności łączone są w celu stworzenia trzech par przeciwstawnych kolorów (\emph{ang. opponent colors}) z teorii barw przeciwstawnych Ewalda Heringa opisanej w \emph{Outlines of a Theory of the Light Sense}.\cite{HERING64} Niemiecki fizjolog Ewald Hering opisał cztery podstawowe kolory: czerwony, zielony, niebieski i żółty, ponadto wyróżnił dodatkowy składnik określony jako intensywność. Poniżej przedstawiono wzory na wyliczenie reprezentacji poszczególnych kanałów:
\begin{equation} 
\label{gist_hering_R} 
R = \frac{r - (g + b)}{2}
\end{equation}
\begin{equation} 
\label{gist_hering_G} 
G = \frac{g - (r + b)}{2}
\end{equation}
\begin{equation} 
\label{gist_hering_B} 
B = \frac{b - (r + g)}{2}
\end{equation}
\begin{equation} 
\label{gist_hering_Y} 
Y = r + g - 2 (|r - g| + b)
\end{equation}
\begin{equation} 
\label{gist_hering_I} 
I = \frac{r + g + b}{3}
\end{equation}

$R, G, B$ -- kanały czerwony, zielony i niebieski według Heringa, $r, g, b$ -- kanały czerwony, zielony i niebieski ze zdjęcia.

Pary kolorów przeciwstawnych tworzą kanały: czerwony i zielony, niebieski i żółty, dodatkowo wyróżniona jest para kanałów: ciemny i jasny. Kolory przeciwstawne nigdy nie są postrzegane razem, tj. nie ma czerwonawej zieleni ani niebieskawej żółci. Kanały kolorów przeciwstawnych zostały przedstawione na rys. \ref{fig:gist-opponent-colors}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.0]{graphics/01_podstawy_teoretyczne/gist-opponent-colors.pdf}
	\caption{ Pary kanałów kolorów przeciwstawnych (\emph{ang. opponent colors}) }
	\label{fig:gist-opponent-colors}
\end{figure}

Poszczególne pary można zapisać w postaci:
\begin{equation} 
\label{gist_siegian_RG} 
RG(c,s) = |(R(c) - G(c)) \ominus (R(s) - G(s))|
\end{equation}
\begin{equation} 
\label{gist_siegian_BY} 
BY(c, s) = |(B(c) - Y(c)) \ominus (B(s) - Y(s))|
\end{equation}
\begin{equation} 
\label{gist_siegian_I} 
I(c, s) = |I(c) \ominus I(s)|
\end{equation}

Każda z par jest używana do stworzenia sześciu kombinacji skal $s$ oraz $c$, co daje 18 kombinacji. Łącznie z kanałami orientacji otrzymujemy zatem 38 podkanałów.

Dla każdego podkanału wyliczany jest wektor GIST z odpowiadających map cech. Mapy dzielone są na podregiony 4x4, a następnie dla każdego podregionu wykonywane są operacje uśredniające. Wzór \ref{gist_siegian_G_kli} przedstawia obliczenia jakie należy wykonać dla każdej z 16 cech GIST na mapę dla koloru i intensywności. Analogiczne obliczenia stosuje się do wyliczenia $G_i^{k,l}(c)$ z map orientacji $M_i(c)$.
\begin{equation} 
\label{gist_siegian_G_kli} 
G_i^{k,l} = \frac{1}{16WH} \sum\limits_{u = \frac{kW}{4}}^{\frac{(k+1)W}{4} - 1} \sum\limits_{v = \frac{lH}{4}}^{\frac{(l+1)H}{4} - 1} [M_i(c, s)](u, v)
\end{equation} gdzie $k$ i $l$ -- indeksy poziomy i pionowy podregionu, $W$, $H$ -- szerokość i wysokość całego obrazu.

Łącznie map cech jest 34, co przy podziale na 16 regionów daje wektor o 544 wymiarach. W celu zmniejszenia wymiarowości stosuje się analizę głównych składowych -- PCA (\emph{ang. Principal Component Analysis}) oraz analizę składowych niezależnych -- ICA (\emph{ang. Independent Component Analysis}). Dzięki temu otrzymuje się wektor 80 wymiarowy.

Schemat procesu ekstrakcji cech GIST według Siegiana i Itti przedstawiono na rys. \ref{fig:gist-siagian}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.0]{graphics/01_podstawy_teoretyczne/gist-siagian.pdf}
	\caption{ Ekstrakcja cech GIST według Siegiana i Itti (2007) \cite{SIAGIAN07} }
	\label{fig:gist-siagian}
\end{figure}

Szybkość obliczeń rozwiązania zaproponowanego przez Siegiana i Itti jest podobna do rozwiązania Olivy i Torralbala z 2000 roku.

\subsection{LBP -- Local Binary Patterns}

Lokalny wzorzec binarny (\emph{ang. Local Binary Pattern, LBP}) jest bezparametrowym operatorem, który opisuje lokalną strukturę przestrzenną obrazu. Został zaproponowany po raz pierwszy przez Timo Ojala w \emph{A comparative study of texture measures with classification based on featured distributions}\cite{OJALA94} z 1994 roku, jako wysoce dyskryminujący dla klasyfikacji tekstur.

LBP dla pozycji $(x_c, y_c)$ zdefiniowany jest jako uporządkowany zestaw binarnych porównań intensywności pikseli pomiędzy centralnym pikselem a ośmioma pikselami otaczającymi w masce 3x3. Przykładowy LBP zaprezentowano na rys. \ref{fig:lbp-operator}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.0]{graphics/01_podstawy_teoretyczne/lbp-operator.pdf}
	\caption{ Podstawowy operator LBP \cite{HEUSCH06} }
	\label{fig:lbp-operator}
\end{figure}

Wartość powstałego kodu LBP można wyrazić w postaci dziesiętnej jako:
\begin{equation} 
\label{lbp_lbp} 
LBP(x_c, y_c) = \sum\limits_{n = 0}^7 s(i_n - i_c) 2^n
\end{equation} gdzie $i_c$ -- wartość piksela $(x_c, y_c)$ w skali szarości, $i_n$ -- wartości 8 otaczających pikseli w skali szarości, natomiast funkcja $s$ zdefiniowana jest następująco:
\begin{equation} 
\label{lbp_s} 
s(x) = 
	\begin{cases}
		1 & \text{ jeżeli } x \geq 0 \\
		0 & \text{ jeżeli } x < 0
	\end{cases} 
\end{equation}

Każdy bit LBP jest w takim samym stopniu ważny, ale dwa dodatnie bity mogą mieć całkowicie inne znaczenie. Operator LBP nie zmienia się poprzez zastosowanie jakichkolwiek monotonicznych transformacji na skali szarości, ponieważ zachowują one porządek intensywności pikseli w lokalnym sąsiedztwie.

W 2002 roku, Ojala rozszerzył oryginalny operator LBP do okrągłego sąsiedztwa o różnych wielkościach promienia.\cite{OJALA02} Nową reprezentację określa się jako $LBP_{P, R}$:
\begin{equation} 
\label{lbp_lbp_pr} 
LBP_{P, R} = \sum\limits_{p=0}^{P-1} s(g_p - g_c) 2^P
\end{equation} gdzie $P$ -- ilość równo rozmieszczonych pikseli, $R$ -- promień, $g_c$ --  wartość centralnego piksela w skali szarości, $g_p (P = 0, \ldots, P-1)$ -- wartości pikseli z sąsiedztwa.

Na rys. \ref{fig:lbp-pr-operator} przedstawiono rozszerzony operator LBP dla sąsiedztwa $(8, 2)$, tj. $P = 8$, $R = 2$.

\begin{figure}[h]
	\centering
	\includegraphics[scale=2.0]{graphics/01_podstawy_teoretyczne/lbp-pr-operator.pdf}
	\caption{ Rozszerzony operator LBP z sąsiedztwem $(8, 2)$ \cite{HEUSCH06} }
	\label{fig:lbp-pr-operator}
\end{figure}

Ponadto zauważono, że większość informacji była zawarta w niewielkim podzbiorze wzorców LBP. Wzorce te zostały nazwane wzorcami jednolitymi (\emph{ang. uniform patterns}) i zawierają co najwyżej dwa przejścia z 0 na 1 lub z 1 na 0 dla kolejnych bitów we wzorcu. 

Dla przykładu $11111111$, $00000000$, $10000111$ i $00001110$ są wzorcami jednolitymi. Natomiast $00011011$, $10001101$, $01010101$ i $01001110$ nie są wzorcami jednolitymi. Przykłady wzorców jednolitych znajdują się również na rys. \ref{fig:lbp-uniform-patterns}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.8]{graphics/01_podstawy_teoretyczne/lbp-uniform-patterns.pdf}
	\caption{ Przykładowe 8 bitowe wzorce jednolite }
	\label{fig:lbp-uniform-patterns}
\end{figure}

Nową postać operatora można zapisać jako:
\begin{equation} 
\label{lbp_lbp_riu2} 
LPR_{P, R}^{riu2} = 
	\begin{cases}
		\sum\limits_{p=0}^{P-1} s(g_p - g_c) & \text{ jeżeli } U(LBP_{P, R}) \leq 2 \\
		P + 1 & \text{ jeżeli } U(LBP_{P, R}) > 2
	\end{cases}
\end{equation} gdzie: \begin{equation} 
\label{lbp_u_lbp} 
U(LBP_{P, R}) = | s(g_{P-1} - g_c) -s(g_0 - g_c) | + \sum\limits_{p=1}^{P-1} | s(g_p - g_c) - s(g_{p-1} - g_c) |
\end{equation}

Ze względu na jego niski koszt obliczeniowy, często wykorzystywany jest do rozwiązywania problemu rozpoznawania twarzy\cite{AHONEN04}\cite{HEUSCH06}\cite{MATURANA09}, wykrywania ruchu\cite{HEIKKILA04}, a także normalizacji obrazów\cite{JUST06}.


\subsection{HOG -- Histograms of Oriented Gradients}

Histogram zorientowanych gradientów (\emph{ang. Histogram of Oriented Gradients, HOG}) został zaproponowany przez Dalala jako jednowymiarowy histogram gradientów orientacji w lokalnych regionach obrazu i zastosowany do detekcji ludzi na fotografiach.\cite{DALAL05} HOG jest odporny na wpływ lokalnych warunków oświetleniowych oraz zmiany geometrii.\cite{YAMAUCHI11}

Gradient obrazu jest otrzymywany poprzez filtrowanie jednowymiarowymi filtrami $[-1, 0, 1]$ (poziomo) oraz $[-1, 0, 1]^T$ (pionowo). Próba zastosowania większych i bardziej skomplikowanych masek dawała gorsze rezultaty. Dla obrazów kolorowych gradienty są obliczane osobno dla każdego kanału.

Następnym krokiem jest tworzenie histogramów gradientów dla komórek. Dla każdego piksela w komórce wyznaczany jest ważony głos dla kanału gradientu orientacji, opartego na wartościach znalezionych w gradiencie dla danego punktu. Komórki mogą być zarówno prostokątne jak i okrągłe. Kanały gradientu mogą być równo rozłożone w zakresie od $0$ do $180$ stopni lub od $0$ do $360$ stopni. Według Dalala i Tiggsa zakres od $0$ do $180$ w połączeniu z zastosowaniem 9 kanałów histogramu sprawdza się najlepiej do rozwiązania problemu detekcji człowieka na zdjęciu.

W celu normalizacji histogramów, komórki łączone są w bloki. Wyróżniamy bloki prostokątne R-HOG oraz okrągłe C-HOG. Bloki C-HOG mogą mieć dwa zasadnicze warianty w zależności od tego czy środkowa komórka jest podzielona czy nie. Na rys. \ref{fig:hog-blocks} przedstawiono przykładowe bloki HOG. Zazwyczaj bloki nachodzą na siebie, tj. pojedynczy piksel może należeć do kilku bloków.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.0]{graphics/01_podstawy_teoretyczne/hog-blocks.pdf}
	\caption{ Bloki HOG: (a) R-HOG, (b) C-HOG z pojedynczą centralną komórką, (c) C-HOG z centralną komórką podzieloną kątowo }
	\label{fig:hog-blocks}
\end{figure}

Bloki mogą być normalizowane z wykorzystaniem trzech metod:
\begin{equation} 
\label{hog_l2_norm} 
f = \frac{\boldsymbol{v}}{\sqrt{ {\Vert \boldsymbol{v} \Vert}_2^2 + e^2 }}
\end{equation}
\begin{equation} 
\label{hog_l1_norm} 
f = \frac{\boldsymbol{v}}{ {\Vert \boldsymbol{v} \Vert}_1 + e }
\end{equation}
\begin{equation} 
\label{hog_l1_sqrt} 
f = \sqrt{\frac{\boldsymbol{v}}{ {\Vert \boldsymbol{v} \Vert}_1 + e }}
\end{equation} gdzie $\boldsymbol{v}$ -- nieznormalizowany wektor zawierający wszystkie histogramy w bloku, ${\Vert \boldsymbol{v} \Vert}_k$ -- norma, $k = 1, 2$, $e$ -- niewielka stała wartość.

Oraz dodatkowym wariantem metody \ref{hog_l2_norm}, w którym maksymalna wartość $\boldsymbol{v}$ jest ograniczana do $0.2$ oraz ponownie normalizowana do długości jednostkowej.

Po normalizacji następuje łączenie histogramów z wszystkich bloków w jeden histogram. Uproszczony schemat działania HOG znajduje się na rys. \ref{fig:hog-process}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.7]{graphics/01_podstawy_teoretyczne/hog-process.pdf}
	\caption{ Tworzenie deskryptora HOG: (a) orientacja gradientów, (b) tworzenie histogramów dla bloków (c) tworzenie wektora cech }
	\label{fig:hog-process}
\end{figure}

Deskryptory HOG operują na komórkach o określonej lokalizacji, stąd metoda ta zachowuje niezmienniczość względem transformacji geometrycznych oraz fotometrycznych, z wyjątkiem orientacji obiektu.\cite{DALAL05}

\section{Klasyfikacja}
Klasyfikacja jest jedną z najczęściej stosowanych metod analizy danych wywodząca się z uczenia maszynowego nadzorowanego. Jej celem jest znalezienie modelu klasyfikacyjnego na podstawie danych historycznych, a następnie zastosowanie odkrytego modelu do predykcji klas nowego obiektu dla którego klasa nie jest znana.

Zbiorem danych wejściowych dla procesu klasyfikacji jest zbiór rekordów w postaci wektorów cech oraz atrybutu decyzyjnego. Atrybut decyzyjny jest atrybutem kategorycznym (nieciągłym). Wartości atrybutu decyzyjnego dzielą zbiór rekordów na predefiniowane klasy.

Celem klasyfikacji jest znalezienie funkcji klasyfikacyjnej, określanej też jako model klasyfikacyjny, który odwzorowuje każdy rekord danych w etykietę klasy. Proces ten został pokazany na rys. \ref{fig:classification-model}. Model klasyfikacyjny często nazywamy też klasyfikatorem.\cite{MORZY13}

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.0]{graphics/01_podstawy_teoretyczne/classification-model.pdf}
	\caption{ Konstruowanie modelu klasyfikacyjnego \cite{MORZY13} }
	\label{fig:classification-model}
\end{figure}

Do klasyfikacji obrazów najczęściej wykorzystywano: naiwny klasyfikator bayesowski, klasyfikator Ada Boost, maszynę wektorów wspierających (SVM) oraz k-najbliższych sąsiadów. Metody te zostały opisane w poniższych podrozdziałach.
	
\subsection{Naiwny klasyfikator bayesowski}
Naiwny klasyfikator bayesowski jest prostym probabilistycznym klasyfikatorem opartym na twierdzeniu Bayesa, który zakłada wzajemną niezależność predykatorów. Założenie o warunkowej niezależności atrybutów można sformułować następująco:
\begin{equation} 
\label{bayes_p_x_c} 
P(X | C = C_i) = \prod\limits_{i = 1}^n P( A_i = x_i | C = C_i )
\end{equation} gdzie $X = (A_1 = x_1, A_2 = x_2, \cdots, A_n = x_n)$ -- rekordy danych, $A = (A_1, A_2, \cdots, A_n)$ -- zbiór atrybutów warunkowych.

Przyjęcie założenia o warunkowej niezależności atrybutów uwalnia klasyfikator bayesowski od kosztownego obliczania prawdopodobieństwa $P(X | C = C_i)$ dla wszystkich kombinacji wartości atrybutów warunkowych $A$. Obliczenia te zastępowane są oszacowaniem warunkowego prawdopodobieństwa wystąpienia wartości $x_j$ atrybutu $A_j, j = 1, \cdots, n$ dla danej klasy $C = C_i$. Obliczeniowo jest to znacznie prostsze i nie wymaga dużych zbiorów treningowych w celu uzyskania poprawnego estymatora prawdopodobieństwa $P(X | C = C_i)$.

Naiwny klasyfikator bayesowski przypisuje rekord $X$ do klasy $C_i$, która maksymalizuje wartość iloczynu $P(X | C = C_i) P(C = C_i)$, co z założeniem o warunkowej niezależności atrybutów (\ref{bayes_p_x_c}) daje:
\begin{equation} 
\label{bayes_C} 
C = \text{arg max } P(C = C_i) \prod\limits_{j = 1}^n P(A_j = x_j | C = C_j)
\end{equation}

Prawdopodobieństwo a priori $P(C = C_i)$ można oszacować jako stosunek liczby rekordów zbioru treningowego należących do klasy $C_i$ do liczby wszystkich rekordów zbioru treningowego, co zapisujemy jako:
\begin{equation} 
\label{bayes_C_n} 
P(C = C_i) = \frac{s_i}{n}
\end{equation} gdzie $s_i$ -- ilość rekordów w klasie $C_i$, n -- ilość wszystkich rekordów.

Metoda oszacowania prawdopodobieństwa warunkowego $P(A_j = x_j | C = C_i)$ zależy od typu atrybutu $A_j$. Jeżeli $A_j$ jest atrybutem kategorycznym to prawdopodobieństwo warunkowe szacujemy jako stosunek liczby rekordów zbioru treningowego należącego do klasy $C_i$, dla którego atrybut $A_j$ przyjmuje wartość $x_j$, do liczby rekordów zbioru treningowego należących do klasy $C_i$:
\begin{equation} 
\label{bayes_PAC} 
P(A_j = x_j | C = C_j) = \frac{s_{ij}}{s_i}
\end{equation} gdzie $s_{ij}$ -- liczba rekordów zbioru treningowego należących do klasy $C_i$, dla których atrybut $A_j$ przyjmuje wartość $x_j$, $s_i$ -- liczba rekordów zbioru treningowego należących do klasy $C_i$.

Jeśli natomiast atrybut $A_j$ jest atrybutem ciągłym można zdyskredytować atrybuty ciągłe i zastąpić wartości atrybutu ciągłego odpowiadającym mu dyskretnym przedziałem wartości, lub założyć standardowy rozkład prawdopodobieństwa wartości atrybutu ciągłego i zastąpić prawdopodobieństwo warunkowe atrybutów odpowiednią funkcją gęstości.

\subsection{Sztuczne sieci neuronowe}
W latach czterdziestych XX wieku zrodził się pomysł naśladowania sposobu przetwarzania i przesyłania informacji przez neurony, co dało początek sztucznym sieciom neuronowym. Podobnie jak w przypadku ich biologicznych odpowiedników, podstawowymi elementami, z których buduje się sztuczne sieci neuronowe, są sztuczne neurony, nazywane też dla uproszczenia neuronami.

Zazwyczaj neurony są rozmieszczone w warstwach. Pierwsza warstwa nazywana jest warstwą wejściową i odpowiada za wprowadzanie danych do sieci. Liczba neuronów w tej warstwie jest równa liczbie wartości wprowadzanych jednocześnie do sieci.

Ostatnia warstwa neuronów to warstwa wyjściowa, która służy do wyznaczania wartości wyjściowych sieci. Pomiędzy warstwą wejściową i wyjściową mogą znajdować się warstwy ukryte, w których realizowane są kolejne etapy przetwarzania informacji wejściowych w informacje wyjściowe.

Neurony znajdujące się w sąsiednich warstwach są ze sobą połączone tworząc ścieżki, po których przesyłane są dane w sieci. Na rys. \ref{fig:ssn-przykladowa-siec} znajduje się przykładowa struktura sztucznej sieci neuronowej dwuwarstwowej. W nazwie sieci podaje się łączną liczbę warstw ukrytych i wyjściowych. 

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.0]{graphics/01_podstawy_teoretyczne/ssn-przykladowa-siec.pdf}
	\caption{ Przykładowa struktura sztucznej sieci neuronowej dwuwarstwowej }
	\label{fig:ssn-przykladowa-siec}
\end{figure}

Na rys. \ref{fig:ssn-neuron} przedstawiono model typowego neuronu. Neuron ten składa się z dwóch bloków: bloku sumowania $\Sigma$ oraz bloku aktywacji $F$.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.0]{graphics/01_podstawy_teoretyczne/ssn-neuron.pdf}
	\caption{ Model sztucznego neuronu }
	\label{fig:ssn-neuron}
\end{figure}

Neuron przetwarza wektor $\boldsymbol{x} = [x_i]$ o wymiarze $n \times 1$, składający się z sygnałów wejściowych $x_i$, na jeden sygnał wyjściowy $y$. Na podstawie danych $x_i$, w bloku sumowania $\Sigma$ obliczana jest wartość $\phi = \vartheta(x)$ funkcji agregującej $\vartheta$, która w najprostszym przypadku jest kombinacją liniową sygnałów wejściowych $x_i$:
\begin{equation} 
\label{ssn_vartheta} 
\vartheta(x) = \sum\limits_{i = 1}^n w_ix_i = \boldsymbol{w}^T\boldsymbol{x}
\end{equation} gdzie $\boldsymbol{x} = [x_i]$ -- wektor $n \times 1$ sygnałów wejściowych, $\boldsymbol{w} = [w_i]$ -- wektor $n \times 1$ współczynników rzeczywistych, które nazywamy wagami. Wagi z jednej strony wyrażają stopień ważności informacji, a z drugiej stanowią pamięć neuronu.

Następnie sygnał $\vartheta$ jest przetwarzany w bloku aktywacji $F$, który na wyjściu daje sygnał $y$. W najprostszym przypadku funkcja aktywacji przyjmuje postać:
\begin{equation} 
\label{ssn_y} 
y = \begin{cases}
		1 & \text{ jeżeli } \phi > \phi_k \\
		0 & \text{ jeżeli } x \leq \phi_k
	\end{cases} 
\end{equation} gdzie $\phi_k$ -- wartość progowa.

Wykorzystanie sztucznej sieci neuronowej do rozwiązywania problemu np. rozpoznawania obrazów lub klasyfikacji, wymaga wyznaczenia wag połączeń pomiędzy neuronami sąsiednich warstw. Wyznaczenie wag w tym przypadku określa się jako uczenie i odbywa się na poziomie poszczególnych neuronów.

Sztuczne sieci neuronowe można wykorzystać do uczenia nadzorowanego poprzez porównywanie otrzymanych wartości z wartościami oczekiwanymi. Wyliczony w ten sposób błąd jest następnie minimalizowany za pomocą algorytmu uczącego, np. z zastosowaniem propagacji wstecznej. Metoda ta polega na przesyłaniu błędów uczenia powstałych na wyjściu sieci od warstwy wyjściowej do wejściowej.
	
\subsection{Klasyfikator k-najbliższych sąsiadów}
Klasyfikator k-najbliższych sąsiadów (\emph{ang. k-nearest neighbors, k-NN}) należy do leniwych metod uczących, jest to zarazem jeden z najprostszych algorytmów uczenia maszynowego. W procesie uczenia nie jest budowany ogólny model klasyfikacyjny, natomiast do klasyfikacji nowego obiektu, o wejściowym wektorze $y$, analizowane są $k$ najbliższe obiekty, a następnie $y$ jest przydzielany do klasy, w której jest większość spośród $k$ obiektów.\cite{HAND01}

Przykład klasyfikacji k-NN dla różnych wartości $k$ znajduje się na rys. \ref{fig:knn-example}. Dla $k=3$ (przerywalna linia), obiekt zostaje zaklasyfikowany do klasy czerwonych trójkątów, ponieważ wśród trzech najbliższych obiektów najwięcej jest czerwonych trójkątów. Analogicznie dla $k=5$, obiekt zostaje zaklasyfikowany do niebieskich kwadratów.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.0]{graphics/01_podstawy_teoretyczne/knn-example.pdf}
	\caption{ Klasyfikacja k-NN dla dwóch różnych wartości $k$ \cite{AJANKI07} }
	\label{fig:knn-example}
\end{figure}

Wynik klasyfikacji k-NN zależy mocno od dwóch czynników: od tego jaką przyjmujemy miarę odległości oraz od tego w jaki sposób dokonujemy transformacji obiektu do reprezentacji w przestrzeni wzorców.\cite{MORZY13}

W wielu zastosowaniach przyjmowana jest metryka euklidesowa, tj. jeśli $\boldsymbol{y}$ jest wejściowym wektorem elementu, który ma być sklasyfikowany, a $\boldsymbol{x}$ jest wektorem wejściowym dla elementu ze zbioru treningowego, to wówczas odległość euklidesowa pomiędzy nimi wynosi:
\begin{equation} 
\label{knn-euclides} 
d(x, y) = \sum_j (x_j - y_j)^2
\end{equation}

Metodę k-NN łatwo jest zaprogramować i nie wymaga ona żadnej optymalizacji ani trenowania. Pozwala na łatwe zastosowanie opcji odrzucania, w której decyzja o klasyfikacji jest odraczana do momentu w którym będziemy wystarczająco przekonani co do przewidzianej klasy. Największa wada metody k-NN wynika z faktu, że nie jest budowany ogólny model klasyfikacyjny, a zamiast tego zapamiętywane są wszystkie punkty zbioru danych treningowych. Jeśli zatem zbiór treningowy jest duży, to przeszukiwanie go w celu znalezienia $k$ najbliższych punktów może okazać się trudne obliczeniowo.\cite{HAND01}

\subsection{Metoda AdaBoost}
Metoda wzmacniania klasyfikatorów (\emph{ang. boosting}) polega na łączeniu odpowiedzi kilku słabych klasyfikatorów w celu stworzenia jednego silnego klasyfikatora. Najpopularniejszym algorytmem realizujących to zadanie jest Adaptive Boosting (w skrócie AdaBoost), meta-algorytm zaproponowany przez Yoava Freunda i Roberta Schapire.\cite{FREUND97} Algorytm ten ma solidne podstawy teoretyczne i wyróżnia się bardzo dobrą predykcją oraz prostotą. Schapire zwykł mawiać, że do jego napisania "wystarczy 10 linii kodu".

Z każdą iteracją, AdaBoost wywołuje prosty algorytm uczący, który zwraca klasyfikator i przypisuje współczynniki wagowe do klasyfikatorów. Końcowa klasyfikacja jest tworzona na podstawie ważonego głosowania klasyfikatorów bazowych. Im mniejszy błąd klasyfikatora bazowego, tym wyższa jego waga w końcowym głosowaniu. Klasyfikatory bazowe muszą być tylko nieznacznie lepsze niż przypadkowe odgadnięcie prawidłowej klasy.\cite{KEGL09}

Algorytm \ref{alg:adaboost} zawiera procedurę AdaBoost przyjmującą zbiór treningowy $\mathcal{D}_n=\{(\boldsymbol{x}_1, y_1),\cdots,\allowbreak(\boldsymbol{x}_n, y_n)\}$, algorytm uczący $\text{Base}(\cdot, \cdot)$ oraz ustaloną liczbę iteracji $T$.\cite{KEGL09}

\begin{algorithm}
	\caption{AdaBoost}
	\label{alg:adaboost}
	\begin{algorithmic}[1]
		\Procedure{AdaBoost}{$\mathcal{D}_n, \text{Base}(\cdot, \cdot), T$}
			\State $w \gets (1/n, ..., 1/n)$ 
			\For{$t \gets 1, T$}
				\State $h^{(t)} \gets \text{Base}(\mathcal{D}_n, \boldsymbol{w}^{(t)})$ \Comment{klasyfikator bazowy}
				\State $\varepsilon^{(t)} \gets \sum\limits_{i = 1}^n w_i^{(t)} \mathbbm{1} \{ h^{t}(\boldsymbol{x}_i) \neq y_i \}$ \Comment{błąd ważony klasyfikatora bazowego}
				\State $\alpha^{(t)} \gets \frac{1}{2} \ln( \frac{1-\varepsilon^{(t)}}{\varepsilon^{(t)}} )$ \Comment{współczynnik klasyfikatora bazowego}
				\For{$i \gets 1$, $n$} \Comment{ponowne ważenie punktów treningowych}
					\If{$h^{(t)}(\boldsymbol{x}_i) \neq y_i$} \Comment{błąd}
						\State $w_i^{(t+1)} \leftarrow \frac{w_i^{(t)}}{2\varepsilon^{(t)}}$ \Comment{zwiększenie wagi}
					\Else \Comment{prawidłowa klasyfikacja}
						\State $w_i^{(t+1)} \leftarrow \frac{w_i^{(t)}}{ 2(1 - \varepsilon^{(t)} )}$ \Comment{zmniejszenie wagi}
					\EndIf
				\EndFor
			\EndFor
			\State \textbf{return} $f^{(T)}(\cdot) = \sum\limits_{t = 1}^T \alpha^{(t)}h^{(t)}(\cdot)$ \Comment{głosowanie ważone klasyfikatorów bazowych}
		\EndProcedure
    \end{algorithmic}
\end{algorithm}

Dla każdej iteracji $t = 1, \cdots, T$, wybieramy klasyfikator $h^{(t)}$ ze zbioru $\mathcal{H}$ klasyfikatorów oraz wyznaczamy współczynnik $\alpha^{(t)}$. W najprostszej wersji tego algorytmu, $\mathcal{H}$ jest skończonym zbiorem binarnych klasyfikatorów w postaci $h = \mathbb{R}^d \rightarrow \{ -1, 1 \}$. Procedura na wyjściu dostarcza funkcję dyskryminującą postaci:
\begin{equation} 
\label{adaboost-discriminant} 
f^{(T)}(\cdot) = \sum_{t=1}^T \alpha^{(t)} h^{(t)}(\cdot)
\end{equation}

Klasyfikację $\boldsymbol{x}$ zapisujemy jako $f^{(T)}(\boldsymbol{x})$. Użyty w algorytmie symbol $\mathbbm{1}\{A\}$ oznacza funkcję charakterystyczną zbioru, która przyjmuje wartość $1$ gdy $A$ jest prawdziwe, a w przeciwnym przypadku wartość $0$.

\subsection{Maszyna wektorów wspierających (SVM)}
Maszyna wektorów nośnych (\emph{ang. Support Vector Machine, SVM}) to bardzo często wykorzystywany w zastosowaniach związanych z analizą obrazów klasyfikator, który wyznacza hiperpłaszczyznę rozdzielającą przykłady należące do dwóch klas z maksymalnym możliwym marginesem.

Pierwotnie metoda ta była wykorzystywana do klasyfikacji wyłącznie separowanych liniowo klas obiektów. Niech w przestrzeni danych $\Omega$ znajdują się wektory danych $\boldsymbol{x}$, stanowiące próbę uczącą $D$, należące do dwóch klas:
\begin{equation} 
\label{svm-D-of-two-classes} 
D = {\{ (\boldsymbol{x}_i, c_i) \in \mathbb{R}^p, c_i \in \{-1, 1\}  \}}_{i=1}^N
\end{equation}

Dwie klasy są liniowo separowane wówczas, gdy istnieje hiperpłaszczyzna $H$ postaci $g(x)$:
\begin{equation} 
\label{svm-linear-separate} 
g(\boldsymbol{x}) = \boldsymbol{w}^T \boldsymbol{x} + b
\end{equation} oraz przyjmuje wartości:
\begin{equation} 
\label{svm-gx} 
\begin{cases}
	g(\boldsymbol{x}_i) > 0 & \text{ jeżeli } \boldsymbol{x} \in 1 \\
	g(\boldsymbol{x}_i) < 0 & \text{ jeżeli } \boldsymbol{x} \in -1
\end{cases}
\end{equation}

Na rys. \ref{fig:svm-margin} przedstawiono przykład hiperpłaszczyzny o maksymalnej separacji, z maksymalnym marginesem separacji $\delta$. 

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.0]{graphics/01_podstawy_teoretyczne/svm-margin.pdf}
	\caption{ Maksymalna separacja hiperpłaszczyzny }
	\label{fig:svm-margin}
\end{figure}

Hiperpłaszczyzny $H_1$ oraz $H_2$ są zdefiniowane przy pomocy obiektów znajdujących się na ich brzegach. Obiekty te nazywamy wektorami nośnymi \emph{ang. support vectors}.

SVM może być również stosowany do separacji klas, które nie mogą być separowane klasyfikatorem liniowym. W takich przypadkach, współrzędne obiektów są mapowane do wielowymiarowej przestrzeni cech, za pomocą nieliniowych funkcji, które nazywamy funkcjami cech (\emph{ang. feature functions}) i oznaczamy $\phi$. Przykład takiej transformacji został umieszczony na rys. \ref{fig:svm-phi}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.0]{graphics/01_podstawy_teoretyczne/svm-phi.pdf}
	\caption{ Liniowa separacja w przestrzeni cech }
	\label{fig:svm-phi}
\end{figure}

	
\section{Ocena klasyfikatorów}
...

\section{Problemy kategoryzacji}
% entry-level categories
% ???
...